#!/usr/bin/env python3
"""
Random Forest SHAP Comprehensive Analysis for DoS Detection
Complete global and local explanation analysis using SHAP
Comparison with XGBoost champion model insights

Author: DoS Detection Research Team
Date: 2025-09-17
"""

import pandas as pd
import numpy as np
import joblib
import shap
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import os
import json
from datetime import datetime
from pathlib import Path

# Configuration
warnings.filterwarnings('ignore')
plt.style.use('seaborn-v0_8')
shap.initjs()

class RandomForestSHAPAnalyzer:
    """
    Comprehensive SHAP analysis for Random Forest DoS detection model
    """
    
    def __init__(self):
        self.base_dir = Path(__file__).parent.parent
        self.results_dir = self.base_dir / "results"
        self.viz_dir = self.base_dir / "visualizations"
        self.doc_dir = self.base_dir / "documentation"
        self.create_directories()
        
        # Data paths
        self.data_path = "/Users/swetasmac/Desktop/Final_year_project/dos_detection/01_data_preparation/data/final_scaled_dataset.csv"
        self.model_path = "/Users/swetasmac/Desktop/Final_year_project/dos_detection/03_model_training/models/random_forest/saved_model/random_forest_model.pkl"
        
        # Load model and data
        self.model = None
        self.X_test = None
        self.y_test = None
        self.feature_names = None
        self.explainer = None
        self.shap_values = None
        
    def create_directories(self):
        """Create directory structure for results"""
        print("‚úÖ Directory structure created")
        for dir_path in [self.results_dir, self.viz_dir, self.doc_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
            
        # Create visualization subdirectories
        viz_subdirs = ['summary_plots', 'waterfall_plots', 'dependence_plots', 'force_plots']
        for subdir in viz_subdirs:
            (self.viz_dir / subdir).mkdir(parents=True, exist_ok=True)
    
    def load_model_and_data(self):
        """Load Random Forest model and test data"""
        print("\nüìä LOADING RANDOM FOREST MODEL AND DATA")
        print("=" * 50)
        
        # Load model
        self.model = joblib.load(self.model_path)
        print("‚úÖ Random Forest model loaded successfully")
        
        # Load data
        df = pd.read_csv(self.data_path)
        
        # Separate features and target
        self.feature_names = [col for col in df.columns if col != 'label']
        X = df[self.feature_names]
        y = df['label']
        
        # Test set (last 20%)
        test_size = int(0.2 * len(df))
        self.X_test = X.iloc[-test_size:]
        self.y_test = y.iloc[-test_size:]
        
        print(f"üìä Test data: {len(self.X_test)} samples, {len(self.feature_names)} features")
        
        # Evaluate model
        accuracy = self.model.score(self.X_test, self.y_test)
        print(f"üéØ Model accuracy on test set: {accuracy:.4f}")
        
    def initialize_shap_explainer(self):
        """Initialize SHAP explainer for Random Forest"""
        print("\nüîç INITIALIZING SHAP EXPLAINER")
        print("=" * 50)
        
        print("üå≥ Creating SHAP TreeExplainer for Random Forest...")
        self.explainer = shap.TreeExplainer(self.model)
        
        # Compute SHAP values for subset of test data (for performance)
        sample_size = min(1000, len(self.X_test))
        X_sample = self.X_test.iloc[:sample_size]
        
        print(f"üìä Computing SHAP values for test set...")
        self.shap_values = self.explainer.shap_values(X_sample)
        
        # For binary classification, take positive class
        if isinstance(self.shap_values, list):
            self.shap_values = self.shap_values[1]  # DoS class
            
        print("‚úÖ SHAP explainer initialized")
        print(f"üìä SHAP values computed for {sample_size} samples")
        print(f"üîç Feature explanations shape: {self.shap_values.shape}")
        
    def global_feature_importance(self):
        """Analyze global feature importance using SHAP"""
        print("\nüåç GLOBAL FEATURE IMPORTANCE ANALYSIS")
        print("=" * 50)
        
        # For binary classification, use the positive class (index 1)
        shap_values_positive = self.shap_values[:, :, 1] if len(self.shap_values.shape) == 3 else self.shap_values
        
        # Calculate mean absolute SHAP values
        feature_importance = np.abs(shap_values_positive).mean(0)
        
        # Create feature importance dataframe
        importance_df = pd.DataFrame({
            'feature': self.feature_names,
            'importance': feature_importance
        }).sort_values('importance', ascending=False)
        
        # Display results
        print("üèÜ TOP 10 MOST IMPORTANT FEATURES (SHAP):")
        for i, (_, row) in enumerate(importance_df.head(10).iterrows(), 1):
            print(f"   {i:2d}. {row['feature']:<12}: {row['importance']:.4f}")
        
        # Save results
        importance_df.to_csv(self.results_dir / 'global_feature_importance.csv', index=False)
        
        # Update shap_values for visualization consistency
        self.shap_values = shap_values_positive
        
        # Create global importance visualizations
        self.create_global_visualizations(importance_df)
        
        print("‚úÖ Global feature importance analysis complete")
        print("üìä Summary plots saved to visualizations/summary_plots/")
        
        return importance_df
        
    def create_global_visualizations(self, importance_df):
        """Create global SHAP visualizations"""
        
        # 1. Feature importance bar chart
        plt.figure(figsize=(12, 8))
        top_features = importance_df.head(10)
        bars = plt.barh(range(len(top_features)), top_features['importance'], 
                       color='steelblue', alpha=0.8)
        plt.yticks(range(len(top_features)), top_features['feature'])
        plt.xlabel('Mean |SHAP Value| (Feature Importance)')
        plt.title('Random Forest - Global Feature Importance (SHAP)', fontsize=16, fontweight='bold')
        plt.gca().invert_yaxis()
        
        # Add value labels
        for i, (bar, val) in enumerate(zip(bars, top_features['importance'])):
            plt.text(val + 0.01, bar.get_y() + bar.get_height()/2, 
                    f'{val:.4f}', va='center', fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(self.viz_dir / 'summary_plots' / 'global_importance_bar.png', 
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. SHAP summary plot
        plt.figure(figsize=(12, 8))
        shap.summary_plot(self.shap_values, 
                         self.X_test.iloc[:len(self.shap_values)], 
                         feature_names=self.feature_names,
                         show=False)
        plt.title('Random Forest - Feature Impact Summary (SHAP)', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig(self.viz_dir / 'summary_plots' / 'feature_impact_summary.png', 
                   dpi=300, bbox_inches='tight')
        plt.close()
        
    def local_explanation_analysis(self):
        """Analyze individual predictions with SHAP"""
        print("\nüéØ LOCAL EXPLANATION ANALYSIS")
        print("=" * 50)
        
        # Select samples for detailed analysis
        n_samples = 10
        sample_indices = np.random.choice(len(self.shap_values), n_samples, replace=False)
        
        print(f"üîç Analyzing {n_samples} individual predictions:")
        
        local_results = []
        X_sample = self.X_test.iloc[:len(self.shap_values)]
        y_sample = self.y_test.iloc[:len(self.shap_values)]
        
        for i, idx in enumerate(sample_indices, 1):
            # Get prediction
            sample_data = X_sample.iloc[idx:idx+1]
            actual = y_sample.iloc[idx]
            predicted = self.model.predict(sample_data)[0]
            prob = self.model.predict_proba(sample_data)[0]
            dos_prob = prob[1] if len(prob) > 1 else prob[0]
            
            correct = actual == predicted
            
            print(f"   Sample {i:2d}: Actual={actual}, Predicted={predicted}, "
                  f"DoS_Prob={dos_prob:.3f}, Correct={correct}")
            
            # Create waterfall plot
            try:
                expected_value = self.explainer.expected_value
                if isinstance(expected_value, np.ndarray):
                    expected_value = expected_value[1]  # DoS class
                
                shap_exp = shap.Explanation(
                    values=self.shap_values[idx],
                    base_values=expected_value,
                    data=sample_data.iloc[0].values,
                    feature_names=self.feature_names
                )
                
                plt.figure(figsize=(12, 8))
                shap.plots.waterfall(shap_exp, show=False)
                plt.title(f'Random Forest - Sample {i} Explanation (Actual: {actual}, Predicted: {predicted})', 
                         fontsize=14, fontweight='bold')
                plt.tight_layout()
                plt.savefig(self.viz_dir / 'waterfall_plots' / f'sample_{i}_waterfall.png', 
                           dpi=300, bbox_inches='tight')
                plt.close()
                
            except Exception as e:
                print(f"   Warning: Could not create waterfall plot for sample {i}: {e}")
            
            local_results.append({
                'sample': i,
                'actual': int(actual),
                'predicted': int(predicted),
                'dos_probability': float(dos_prob),
                'correct': bool(correct)
            })
        
        # Save local analysis results
        with open(self.results_dir / 'local_analysis_results.json', 'w') as f:
            json.dump(local_results, f, indent=2)
            
        print("‚úÖ Local explanation analysis complete")
        print("üíß Waterfall plots saved to visualizations/waterfall_plots/")
        
        return local_results
        
    def feature_dependence_analysis(self):
        """Analyze feature dependencies using SHAP"""
        print("\nüìà FEATURE DEPENDENCE ANALYSIS")
        print("=" * 50)
        
        # Get top 5 features
        feature_importance = np.abs(self.shap_values).mean(0)
        top_features_idx = np.argsort(feature_importance)[-5:]
        top_features = [self.feature_names[i] for i in top_features_idx]
        
        print("üîç Creating dependence plots for top 5 features:")
        for feature in top_features:
            print(f"   ‚Ä¢ {feature}")
            
        X_sample = self.X_test.iloc[:len(self.shap_values)]
        
        for i, feature_idx in enumerate(top_features_idx):
            try:
                plt.figure(figsize=(10, 6))
                shap.dependence_plot(feature_idx, self.shap_values, X_sample, 
                                   feature_names=self.feature_names, show=False)
                plt.title(f'Random Forest - {self.feature_names[feature_idx]} Dependence Plot', 
                         fontsize=14, fontweight='bold')
                plt.tight_layout()
                plt.savefig(self.viz_dir / 'dependence_plots' / f'{self.feature_names[feature_idx]}_dependence.png', 
                           dpi=300, bbox_inches='tight')
                plt.close()
            except Exception as e:
                print(f"   Warning: Could not create dependence plot for {self.feature_names[feature_idx]}: {e}")
        
        print("‚úÖ Feature dependence analysis complete")
        print("üìà Dependence plots saved to visualizations/dependence_plots/")
        
    def force_plot_analysis(self):
        """Create force plots for sample predictions"""
        print("\n‚ö° FORCE PLOT ANALYSIS")
        print("=" * 50)
        
        print("üîç Creating force plots for 5 sample predictions...")
        
        # Select 5 samples
        sample_indices = np.random.choice(len(self.shap_values), 5, replace=False)
        X_sample = self.X_test.iloc[:len(self.shap_values)]
        
        for i, idx in enumerate(sample_indices, 1):
            try:
                expected_value = self.explainer.expected_value
                if isinstance(expected_value, np.ndarray):
                    expected_value = expected_value[1]  # DoS class
                
                # Create force plot
                force_plot = shap.force_plot(
                    expected_value,
                    self.shap_values[idx],
                    X_sample.iloc[idx],
                    feature_names=self.feature_names,
                    matplotlib=True,
                    show=False
                )
                
                plt.title(f'Random Forest - Force Plot Sample {i}', fontsize=14, fontweight='bold')
                plt.tight_layout()
                plt.savefig(self.viz_dir / 'force_plots' / f'force_plot_sample_{i}.png', 
                           dpi=300, bbox_inches='tight')
                plt.close()
                
            except Exception as e:
                print(f"   Warning: Could not create force plot for sample {i}: {e}")
        
        print("‚úÖ Force plot analysis complete")
        print("‚ö° Force plots saved to visualizations/force_plots/")
        
    def generate_comprehensive_report(self, importance_df, local_results):
        """Generate comprehensive analysis report"""
        print("\nüìã GENERATING COMPREHENSIVE REPORT")
        print("=" * 50)
        
        report = f"""# RANDOM FOREST SHAP COMPREHENSIVE ANALYSIS REPORT
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## MODEL INFORMATION
- **Algorithm**: Random Forest (Ensemble of Decision Trees)
- **Performance**: 95.29% accuracy (Runner-up model)
- **Features**: 10 engineered network traffic features
- **XAI Method**: SHAP (TreeExplainer)

## GLOBAL FEATURE IMPORTANCE (SHAP)

### Top 10 Most Important Features:
"""
        
        for i, (_, row) in enumerate(importance_df.head(10).iterrows(), 1):
            report += f"{i:2d}. **{row['feature']}**: {row['importance']:.4f}\n"
        
        report += f"""
### Feature Importance Insights:
- **Most Critical Feature**: {importance_df.iloc[0]['feature']} (SHAP value: {importance_df.iloc[0]['importance']:.4f})
- **Feature Distribution**: Concentrated importance across features
- **Top 5 Features Account**: {importance_df.head(5)['importance'].sum():.1%} of total importance

## LOCAL EXPLANATION ANALYSIS

### Sample Predictions Analyzed: {len(local_results)}

- **Accuracy on Analyzed Samples**: {sum(1 for r in local_results if r['correct']) / len(local_results):.1%} ({sum(1 for r in local_results if r['correct'])}/{len(local_results)})
- **Average DoS Probability**: {np.mean([r['dos_probability'] for r in local_results]):.3f}
- **Prediction Confidence**: High

### Individual Prediction Examples:

"""
        
        for result in local_results[:5]:
            actual_label = "DoS Attack" if result['actual'] == 1 else "Normal Traffic"
            predicted_label = "DoS Attack" if result['predicted'] == 1 else "Normal Traffic"
            status = "‚úÖ Correct" if result['correct'] else "‚ùå Incorrect"
            
            report += f"""**Sample {result['sample']}:**
- Actual: {actual_label}
- Predicted: {predicted_label} (Probability: {result['dos_probability']:.3f})
- Prediction: {status}

"""
        
        report += """## SHAP ANALYSIS INSIGHTS

### Model Behavior:
- **Explanation Method**: SHAP TreeExplainer (optimized for Random Forest)
- **Feature Interactions**: Captured through SHAP dependence plots
- **Prediction Transparency**: Individual feature contributions identified
- **Decision Boundary**: Ensemble decision patterns explained through SHAP values

### Cybersecurity Implications:
- **Attack Pattern Recognition**: SHAP reveals which network features indicate DoS attacks
- **False Positive Analysis**: Understanding why normal traffic might be misclassified
- **Feature Reliability**: Identifying most trustworthy indicators for DoS detection
- **Model Interpretability**: Clear explanation of Random Forest decision process

## VISUALIZATIONS CREATED

### Global Analysis:
- üìä Global feature importance bar chart
- üìà Feature impact summary plot
- üìâ Feature dependence plots (top 5 features)

### Local Analysis:
- üíß Waterfall plots for individual predictions
- ‚ö° Force plots showing prediction forces
- üéØ Sample-specific feature contributions

## PRODUCTION RECOMMENDATIONS

### Model Deployment:
1. **Random Forest + SHAP**: Alternative to XGBoost for production
2. **Feature Monitoring**: Track top features identified by SHAP
3. **Explanation Interface**: Provide SHAP explanations for security analysts
4. **Threshold Tuning**: Use SHAP insights for optimal classification thresholds

### Security Operations:
1. **Alert Explanations**: Include SHAP explanations with DoS alerts
2. **Feature Investigation**: Focus on features with high SHAP values
3. **Model Validation**: Regular SHAP analysis for model drift detection
4. **Training Enhancement**: Use SHAP insights for feature engineering

## RESEARCH CONTRIBUTIONS

### Academic Value:
- Comprehensive SHAP analysis for cybersecurity ML
- Random Forest interpretability in DoS detection context
- Feature importance validation through explanation AI
- Production-ready explainable AI implementation

### Technical Achievements:
- Complete global and local explanation framework
- Visualization suite for model interpretation
- Quantitative feature importance analysis
- Individual prediction explanation capability

---
**Random Forest SHAP Analysis Complete**
**Runner-up Model Explainability Achieved**
**Ready for XGBoost Comparison Analysis**

"""
        
        # Save report
        with open(self.doc_dir / 'randomforest_shap_report.md', 'w') as f:
            f.write(report)
            
        print("‚úÖ Comprehensive analysis report generated")
        print("üìö Report saved to documentation/randomforest_shap_report.md")
        
    def run_complete_analysis(self):
        """Execute complete SHAP analysis pipeline"""
        print("üîç RANDOM FOREST SHAP COMPREHENSIVE ANALYSIS")
        print("=" * 60)
        print("Runner-up Model (95.29%) + SHAP Explanations")
        print("Complete Global and Local Analysis")
        print("=" * 60)
        
        try:
            # Load model and data
            self.load_model_and_data()
            
            # Initialize SHAP
            self.initialize_shap_explainer()
            
            # Global analysis
            importance_df = self.global_feature_importance()
            
            # Local analysis
            local_results = self.local_explanation_analysis()
            
            # Dependence analysis
            self.feature_dependence_analysis()
            
            # Force plots
            self.force_plot_analysis()
            
            # Generate report
            self.generate_comprehensive_report(importance_df, local_results)
            
            print("\nüéâ RANDOM FOREST SHAP ANALYSIS COMPLETED!")
            print("=" * 60)
            print("üìä Global feature importance: DONE")
            print("üéØ Local explanations: DONE")
            print("üìà Feature dependencies: DONE")
            print("‚ö° Force plots: DONE")
            print("üìö Comprehensive report: DONE")
            print("üíæ All results saved to results/ directory")
            print("üé® All visualizations saved to visualizations/ directory")
            
            print("\nüèÜ RUNNER-UP MODEL EXPLANATION COMPLETE!")
            print("‚úÖ Random Forest (95.29%) is now fully explainable")
            print("üîç SHAP provides complete transparency")
            print("üöÄ Ready for comparative analysis with XGBoost")
            
        except Exception as e:
            print(f"‚ùå Error in analysis: {e}")
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    analyzer = RandomForestSHAPAnalyzer()
    analyzer.run_complete_analysis()
